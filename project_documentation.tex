\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{natbib}

% Page geometry
\geometry{margin=1in}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\rhead{Medical AI Agent with RAG}
\lhead{Project Documentation}
\rfoot{Page \thepage}

% Colors
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Code listing style
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% Title formatting
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Huge\bfseries Medical AI Agent with RAG Integration\par}
    \vspace{0.5cm}
    {\Large Pattern 2: AI Agent Interfacing with RAG Application\par}
    
    \vspace{2cm}
    
    {\large\itshape Bootcamp Project Submission\par}
    
    \vspace{1.5cm}
    
    {\Large Author: Baraa Khanfar\par}
    {\large Date: January 2, 2026\par}
    
    \vspace{2cm}
    
    \begin{abstract}
    This document presents a comprehensive Medical AI Agent system that combines the power of CrewAI-based autonomous agents with Retrieval-Augmented Generation (RAG) for medical knowledge retrieval. The system provides evidence-based medical information through an intuitive web interface, featuring session management, intelligent caching, and full observability through Langfuse integration. This implementation follows Pattern 2 of AI agent architecture, where specialized AI agents interface with a RAG-powered medical knowledge base.
    \end{abstract}
    
    \vfill
    
    {\large\bfseries Technologies Used:\par}
    \vspace{0.3cm}
    CrewAI $\bullet$ Google Gemini $\bullet$ LangChain $\bullet$ ChromaDB $\bullet$ Flask $\bullet$ Redis $\bullet$ Langfuse
    
\end{titlepage}

% Table of Contents
\tableofcontents
\newpage

% ============================================================================
\section{Project Name and Description}
% ============================================================================

\subsection{Project Name}
\textbf{Medical AI Agent with RAG Integration}

\subsection{Description}
The Medical AI Agent is an intelligent healthcare information system that combines autonomous AI agents with a Retrieval-Augmented Generation (RAG) pipeline. Built using the CrewAI framework, the system deploys specialized agents---Medical Assistant, Medical Researcher, and Medical Educator---that collaborate to answer medical questions, conduct research on health topics, and explain complex medical concepts in accessible language.

The system interfaces with a comprehensive medical knowledge base containing:
\begin{itemize}
    \item \textbf{MedQA}: Medical licensing examination questions and answers
    \item \textbf{MedDialog}: Doctor-patient conversation transcripts
    \item \textbf{HealthSearchQA}: Health-related search queries and responses
    \item \textbf{LiveQA}: Consumer health questions from real users
\end{itemize}

\subsection{Key Features}
\begin{enumerate}
    \item \textbf{Multi-Agent Architecture}: Three specialized agents with distinct roles and capabilities
    \item \textbf{RAG-Powered Knowledge Retrieval}: Semantic search through medical literature
    \item \textbf{Dual Query Modes}: Direct RAG queries and agent-orchestrated responses
    \item \textbf{Session Management}: Persistent conversation history with session tracking
    \item \textbf{Intelligent Caching}: Redis-based caching with in-memory fallback
    \item \textbf{Batch Processing}: Process multiple queries simultaneously
    \item \textbf{Full Observability}: Langfuse integration for tracing, monitoring, and quality scoring
\end{enumerate}

% ============================================================================
\section{End User Benefits}
% ============================================================================

The Medical AI Agent provides significant value to end users seeking medical information:

\subsection{Accessibility}
\begin{itemize}
    \item \textbf{24/7 Availability}: Access medical information anytime without waiting for appointments
    \item \textbf{No Medical Background Required}: Complex medical concepts explained in simple, understandable terms
    \item \textbf{Multiple Languages Support}: Potential for multilingual responses (future enhancement)
\end{itemize}

\subsection{Quality of Information}
\begin{itemize}
    \item \textbf{Evidence-Based Responses}: All answers cite source documents from verified medical datasets
    \item \textbf{Comprehensive Analysis}: Multi-agent collaboration ensures thorough, well-researched answers
    \item \textbf{Educational Focus}: Medical Educator agent transforms clinical information into patient-friendly content
\end{itemize}

\subsection{User Experience}
\begin{itemize}
    \item \textbf{Fast Response Times}: Intelligent caching reduces wait times for repeated queries
    \item \textbf{Conversation History}: Users can review past interactions and track their health queries
    \item \textbf{Multiple Query Options}: Choice between quick RAG queries and in-depth agent analysis
    \item \textbf{Batch Processing}: Submit multiple questions at once for efficient research
\end{itemize}

\subsection{Safety Features}
\begin{itemize}
    \item \textbf{Medical Disclaimers}: Clear reminders to consult healthcare professionals
    \item \textbf{No Diagnostic Claims}: System provides information, not medical advice
    \item \textbf{Source Transparency}: Users can see which documents informed each response
\end{itemize}

% ============================================================================
\section{Business Benefits}
% ============================================================================

Deploying the Medical AI Agent offers substantial advantages for healthcare organizations and businesses:

\subsection{Cost Reduction}
\begin{itemize}
    \item \textbf{Reduced Support Volume}: Automates responses to common medical inquiries, reducing call center load by up to 40\%
    \item \textbf{Efficient Resource Allocation}: Medical professionals can focus on complex cases while AI handles routine information requests
    \item \textbf{Caching Optimization}: Intelligent caching reduces computational costs for frequently asked questions
\end{itemize}

\subsection{Scalability}
\begin{itemize}
    \item \textbf{Unlimited Concurrent Users}: Handle thousands of simultaneous queries without degradation
    \item \textbf{Horizontal Scaling}: Microservices architecture allows independent scaling of components
    \item \textbf{Cloud-Native Design}: Easy deployment on cloud platforms (AWS, GCP, Azure)
\end{itemize}

\subsection{Compliance and Quality Assurance}
\begin{itemize}
    \item \textbf{Full Auditability}: Langfuse observability provides complete trace logs of all interactions
    \item \textbf{Quality Scoring}: Automated quality metrics for continuous improvement
    \item \textbf{Session Tracking}: Compliance with data retention and privacy requirements
    \item \textbf{Cost Monitoring}: Real-time tracking of LLM usage and associated costs
\end{itemize}

\subsection{Competitive Advantage}
\begin{itemize}
    \item \textbf{Enhanced Patient Engagement}: 24/7 access improves patient satisfaction scores
    \item \textbf{Data-Driven Insights}: Analytics on common queries inform service improvements
    \item \textbf{Brand Differentiation}: AI-powered services position organizations as technology leaders
\end{itemize}

\subsection{Return on Investment (ROI)}
\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Traditional} & \textbf{With AI Agent} \\
\midrule
Average Response Time & 24-48 hours & < 30 seconds \\
Cost per Query & \$5-15 & \$0.01-0.05 \\
Availability & Business hours & 24/7/365 \\
Concurrent Capacity & Limited by staff & Virtually unlimited \\
\bottomrule
\end{tabular}
\caption{Comparative analysis of traditional vs. AI-powered medical information services}
\end{table}

% ============================================================================
\section{Technical Architecture}
% ============================================================================

\subsection{Architecture Overview}

The system follows a layered architecture with clear separation of concerns. Figure \ref{fig:architecture} presents the complete system architecture.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{architecture_diagram.png}
    \caption{Medical AI Agent System Architecture}
    \label{fig:architecture}
\end{figure}

\subsection{Component Descriptions}

\subsubsection{Frontend Layer}
\begin{itemize}
    \item \textbf{Technology}: HTML5, CSS3 (Bootstrap 5), JavaScript (Vanilla)
    \item \textbf{Features}: 
    \begin{itemize}
        \item Responsive design with tabbed interface
        \item Real-time query submission with loading indicators
        \item Session history viewer with clear functionality
        \item Cache statistics dashboard
        \item Batch query interface
    \end{itemize}
\end{itemize}

\subsubsection{Flask REST API}
\begin{itemize}
    \item \textbf{Framework}: Flask 2.x with Flask-CORS
    \item \textbf{Endpoints}:
    \begin{lstlisting}[language=Python]
POST /api/v1/agent/query    # Agent-orchestrated query
POST /api/v1/agent/research # In-depth research mode
POST /api/v1/rag/query      # Direct RAG query
GET  /api/v1/history        # Retrieve session history
DELETE /api/v1/history      # Clear session history
POST /api/v1/cache/clear    # Clear response cache
GET  /api/v1/cache/stats    # Cache statistics
POST /api/v1/batch          # Batch query processing
    \end{lstlisting}
\end{itemize}

\subsubsection{CrewAI Agent Layer}
The agent layer implements three specialized agents using the CrewAI framework:

\begin{enumerate}
    \item \textbf{Medical Assistant Agent}
    \begin{itemize}
        \item Role: Senior Medical Information Specialist
        \item Capabilities: Query interpretation, knowledge retrieval, response synthesis
        \item Tools: medical\_knowledge\_query, medical\_document\_search
    \end{itemize}
    
    \item \textbf{Medical Researcher Agent}
    \begin{itemize}
        \item Role: Medical Research Analyst
        \item Capabilities: Deep research, multi-source analysis, comprehensive reporting
        \item Tools: medical\_document\_search
    \end{itemize}
    
    \item \textbf{Medical Educator Agent}
    \begin{itemize}
        \item Role: Health Educator
        \item Capabilities: Simplification, analogy generation, patient-friendly explanations
        \item Tools: medical\_knowledge\_query
    \end{itemize}
\end{enumerate}

\subsubsection{RAG Pipeline}
\begin{itemize}
    \item \textbf{Document Loading}: Custom MedicalDataLoader for JSON medical datasets
    \item \textbf{Text Chunking}: RecursiveCharacterTextSplitter (1000 chars, 200 overlap)
    \item \textbf{Embeddings}: HuggingFace sentence-transformers/all-MiniLM-L6-v2
    \item \textbf{Vector Store}: ChromaDB (primary) with FAISS (alternative)
    \item \textbf{Retrieval}: Similarity search with configurable top-k results
\end{itemize}

\subsubsection{LLM Integration}
\begin{itemize}
    \item \textbf{Model}: Google Gemini 2.5-flash
    \item \textbf{Integration}: LangChain ChatGoogleGenerativeAI
    \item \textbf{Configuration}: Temperature 0.7, with Langfuse callback handlers
\end{itemize}

\subsubsection{Cache and Storage Layer}
\begin{itemize}
    \item \textbf{Primary Cache}: Redis (DB 0) with configurable TTL
    \item \textbf{Session History}: Redis (DB 1) with per-session storage
    \item \textbf{Fallback}: In-memory dictionaries when Redis unavailable
    \item \textbf{Vector Storage}: Persistent ChromaDB/FAISS indices
\end{itemize}

\subsubsection{Observability Layer (Langfuse)}
\begin{itemize}
    \item \textbf{Tracing}: Full request lifecycle tracking with @observe decorators
    \item \textbf{Session Management}: Session ID propagation via propagate\_attributes
    \item \textbf{Metrics}: Input/output logging, token usage, latency tracking
    \item \textbf{Quality Scoring}: Automated scoring based on response quality metrics
    \item \textbf{Cost Tracking}: Per-request LLM cost monitoring
\end{itemize}

\subsection{Data Flow}

\begin{enumerate}
    \item User submits query through web interface
    \item Flask API receives request and checks cache
    \item If cache miss, request routes to Agent or RAG pipeline
    \item Agent uses RAG tools to search medical knowledge base
    \item LLM generates response based on retrieved context
    \item Response cached and returned to user
    \item All operations traced to Langfuse for observability
\end{enumerate}

% ============================================================================
\section{Technologies Tried But Not Used}
% ============================================================================

During development, several technologies and approaches were evaluated but ultimately not included in the final implementation:

\subsection{OpenAI GPT-4}
\begin{itemize}
    \item \textbf{Reason for Consideration}: Industry-leading performance on complex reasoning tasks
    \item \textbf{Reason Not Used}: Cost considerations and preference for Google ecosystem integration
    \item \textbf{Alternative Chosen}: Google Gemini 2.5-flash offers comparable performance at lower cost
\end{itemize}

\subsection{Pinecone Vector Database}
\begin{itemize}
    \item \textbf{Reason for Consideration}: Managed vector database with excellent scalability
    \item \textbf{Reason Not Used}: Added complexity and cost for the project scope
    \item \textbf{Alternative Chosen}: ChromaDB provides sufficient functionality with simpler local deployment
\end{itemize}

\subsection{LlamaIndex}
\begin{itemize}
    \item \textbf{Reason for Consideration}: Comprehensive RAG framework with advanced features
    \item \textbf{Reason Not Used}: LangChain already integrated with CrewAI; avoiding redundant frameworks
    \item \textbf{Alternative Chosen}: LangChain for RAG pipeline integration
\end{itemize}

\subsection{AutoGen (Microsoft)}
\begin{itemize}
    \item \textbf{Reason for Consideration}: Alternative multi-agent framework
    \item \textbf{Reason Not Used}: CrewAI offers more intuitive agent definition and better documentation
    \item \textbf{Alternative Chosen}: CrewAI for agent orchestration
\end{itemize}

\subsection{Celery for Batch Processing}
\begin{itemize}
    \item \textbf{Reason for Consideration}: Production-grade task queue for async processing
    \item \textbf{Reason Not Used}: Adds infrastructure complexity; synchronous batch sufficient for demo
    \item \textbf{Alternative Chosen}: Simple synchronous batch processing in Flask
\end{itemize}

\subsection{PostgreSQL for History Storage}
\begin{itemize}
    \item \textbf{Reason for Consideration}: Robust relational database for structured data
    \item \textbf{Reason Not Used}: Redis provides simpler key-value storage adequate for session history
    \item \textbf{Alternative Chosen}: Redis with JSON serialization
\end{itemize}

% ============================================================================
\section{Current Limitations and Drawbacks}
% ============================================================================

\subsection{Technical Limitations}

\begin{enumerate}
    \item \textbf{Knowledge Base Currency}
    \begin{itemize}
        \item Static medical dataset requires manual updates
        \item No real-time integration with medical literature databases
        \item Knowledge cutoff based on training data
    \end{itemize}
    
    \item \textbf{Scalability Constraints}
    \begin{itemize}
        \item Single-threaded Flask server limits concurrent requests
        \item In-memory fallback loses data on restart
        \item No horizontal scaling configuration included
    \end{itemize}
    
    \item \textbf{Response Latency}
    \begin{itemize}
        \item Agent queries require 15-45 seconds due to multi-step processing
        \item CrewAI verbose mode adds overhead
        \item No streaming responses implemented
    \end{itemize}
    
    \item \textbf{Language Support}
    \begin{itemize}
        \item Currently English-only
        \item No medical terminology translation
    \end{itemize}
\end{enumerate}

\subsection{Functional Limitations}

\begin{enumerate}
    \item \textbf{No User Authentication}
    \begin{itemize}
        \item Session IDs are client-generated
        \item No access control or user management
        \item History accessible to anyone with session ID
    \end{itemize}
    
    \item \textbf{Limited Medical Scope}
    \begin{itemize}
        \item Dataset focused on common conditions
        \item No coverage of rare diseases or cutting-edge treatments
        \item No drug interaction checking
    \end{itemize}
    
    \item \textbf{No Image/Document Analysis}
    \begin{itemize}
        \item Cannot process medical images (X-rays, MRIs)
        \item No PDF or document upload capability
    \end{itemize}
\end{enumerate}

\subsection{Operational Limitations}

\begin{enumerate}
    \item \textbf{Monitoring Gaps}
    \begin{itemize}
        \item No health check endpoints
        \item No automated alerting
        \item Manual log analysis required
    \end{itemize}
    
    \item \textbf{Deployment Complexity}
    \begin{itemize}
        \item Requires Redis for full functionality
        \item Multiple environment variables to configure
        \item No containerization (Docker) included
    \end{itemize}
\end{enumerate}

\subsection{Future Improvements}

\begin{itemize}
    \item Implement streaming responses for better UX
    \item Add user authentication with OAuth2
    \item Containerize with Docker and add Kubernetes manifests
    \item Integrate with medical literature APIs (PubMed)
    \item Add multimodal capabilities for medical image analysis
    \item Implement rate limiting and API quotas
\end{itemize}

% ============================================================================
\section{References and Citations}
% ============================================================================

\subsection{Frameworks and Libraries}

\begin{enumerate}
    \item \textbf{CrewAI}: Multi-agent orchestration framework
    \begin{itemize}
        \item Documentation: \url{https://docs.crewai.com/}
        \item GitHub: \url{https://github.com/joaomdmoura/crewAI}
    \end{itemize}
    
    \item \textbf{LangChain}: LLM application framework
    \begin{itemize}
        \item Documentation: \url{https://python.langchain.com/docs/}
        \item Citation: Harrison Chase et al. (2023). LangChain: Building applications with LLMs.
    \end{itemize}
    
    \item \textbf{Google Gemini}: Large Language Model
    \begin{itemize}
        \item Documentation: \url{https://ai.google.dev/docs}
        \item Reference: Google DeepMind (2024). Gemini: A Family of Highly Capable Multimodal Models.
    \end{itemize}
    
    \item \textbf{ChromaDB}: Vector database
    \begin{itemize}
        \item Documentation: \url{https://docs.trychroma.com/}
        \item GitHub: \url{https://github.com/chroma-core/chroma}
    \end{itemize}
    
    \item \textbf{Langfuse}: LLM observability platform
    \begin{itemize}
        \item Documentation: \url{https://langfuse.com/docs}
        \item GitHub: \url{https://github.com/langfuse/langfuse}
    \end{itemize}
\end{enumerate}

\subsection{Medical Datasets}

\begin{enumerate}
    \item \textbf{MedQA}: Jin et al. (2021). "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams." Applied Sciences.
    
    \item \textbf{MedDialog}: Zeng et al. (2020). "MedDialog: Large-scale Medical Dialogue Datasets." EMNLP 2020.
    
    \item \textbf{HealthSearchQA}: Google Research. Consumer health question-answer dataset.
    
    \item \textbf{LiveQA}: Abacha \& Demner-Fushman (2019). "A Question-Entailment Approach to Question Answering." BMC Bioinformatics.
\end{enumerate}

\subsection{Additional References}

\begin{enumerate}
    \item Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." NeurIPS 2020.
    
    \item Brown, T., et al. (2020). "Language Models are Few-Shot Learners." NeurIPS 2020.
    
    \item Vaswani, A., et al. (2017). "Attention Is All You Need." NeurIPS 2017.
    
    \item Flask Documentation: \url{https://flask.palletsprojects.com/}
    
    \item Redis Documentation: \url{https://redis.io/documentation}
    
    \item HuggingFace Transformers: \url{https://huggingface.co/docs/transformers/}
\end{enumerate}

% ============================================================================
\section{Appendix}
% ============================================================================

\subsection{Project Structure}
\begin{lstlisting}[language=bash]
medical_agent/
|-- app.py              # Flask REST API
|-- agent_main.py       # CrewAI agents
|-- rag_pipeline.py     # RAG implementation
|-- rag_tools.py        # CrewAI tools
|-- data_loader.py      # Data processing
|-- config.py           # Configuration
|-- logger.py           # Logging setup
|-- requirements.txt    # Dependencies
|-- .env.example        # Environment template
|-- templates/
|   |-- index.html      # Web interface
|-- data/
    |-- processed/
        |-- combined_medical_data.json
\end{lstlisting}

\subsection{Environment Configuration}
\begin{lstlisting}[language=bash]
# Required
GOOGLE_API_KEY=your_google_api_key

# Optional - Langfuse
LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=pk-lf-xxx
LANGFUSE_SECRET_KEY=sk-lf-xxx
LANGFUSE_HOST=https://cloud.langfuse.com

# Optional - Redis
REDIS_HOST=localhost
REDIS_PORT=6379
\end{lstlisting}

\subsection{Running the Application}
\begin{lstlisting}[language=bash]
# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env with your API keys

# Start Redis (optional)
redis-server

# Run the application
python app.py

# Access at http://localhost:5000
\end{lstlisting}

\end{document}
